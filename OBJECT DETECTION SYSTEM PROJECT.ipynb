{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **-------------**>**OBJECT DETECTION SYSTEM PROJECT**<**-------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import Libraries and Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "base_dir = r\"D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\"\n",
    "image_dir = os.path.join(base_dir, 'Filtered_Data', 'Filtered_Images')\n",
    "csv_path = os.path.join(base_dir, 'Filtered_Data', 'Humans_Dataset.csv')\n",
    "output_dir = base_dir\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Create Directories and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for train and validation datasets\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'val')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(val_dir, 'images'), exist_ok=True)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "split_ratio = 0.8\n",
    "train_df = df.sample(frac=split_ratio, random_state=42)\n",
    "val_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Image Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess images\n",
    "def preprocess_images(df, source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for img_name in df['image_name'].unique():\n",
    "        img_path = os.path.join(source_dir, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            target_path = os.path.join(target_dir, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                cv2.imwrite(target_path, img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Execute Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "preprocess_images(train_df, image_dir, os.path.join(train_dir, 'images'))\n",
    "preprocess_images(val_df, image_dir, os.path.join(val_dir, 'images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Prepare YAML File for YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare YAML file for YOLO\n",
    "class_names = ['face']\n",
    "train_path = os.path.join(output_dir, 'train', 'images').replace(\"\\\\\", \"/\")\n",
    "val_path = os.path.join(output_dir, 'val', 'images').replace(\"\\\\\", \"/\")\n",
    "data_yaml_content = f\"\"\"\n",
    "train: {train_path}\n",
    "val: {val_path}\n",
    "nc: {len(class_names)}\n",
    "names: {class_names}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Write YAML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data YAML file created at: D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "# Write YAML content to file\n",
    "data_yaml_path = os.path.join(output_dir, 'data.yaml')\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(f\"Data YAML file created at: {data_yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Train YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients, 8.9 GFLOPs\n",
      "\n",
      "New https://pypi.org/project/ultralytics/8.2.100 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.134  Python-3.11.5 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1myolo\\engine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\data.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=True, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=1.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLOv8n summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train', view at http://localhost:6006/\n",
      "c:\\Users\\ENGHK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\train\\labels... 1433 images, 327 backgrounds, 0 corrupt: 100%|██████████| 1760/1760 [00:01<00:00, 929.47it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\val\\labels... 104 images, 470 backgrounds, 0 corrupt: 100%|██████████| 574/574 [00:00<00:00, 1371.77it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\# DATA SCIENCE\\# PROJECTS\\- PROJECTS INTERNSHIPS\\CODECLAUSE -AI ENGINEERING\\Object Detection System Project\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/110 [00:00<?, ?it/s]c:\\Users\\ENGHK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\engine\\trainer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self.amp):\n",
      "       1/10         0G      2.747      4.625      3.889         15        640: 100%|██████████| 110/110 [06:49<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.55s/it]\n",
      "                   all        574        104   0.000356      0.433   0.000271   9.38e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10         0G      2.577      3.431      3.161         13        640: 100%|██████████| 110/110 [06:51<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.51s/it]\n",
      "                   all        574        104    0.00062      0.615   0.000707   0.000189\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10         0G      2.376      2.652      2.962         14        640: 100%|██████████| 110/110 [06:50<00:00,  3.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:44<00:00,  2.50s/it]\n",
      "                   all        574        104     0.0591       0.24     0.0459      0.019\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10         0G      2.219      2.285      2.878         13        640: 100%|██████████| 110/110 [06:49<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.51s/it]\n",
      "                   all        574        104     0.0467      0.442     0.0371     0.0188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10         0G      2.057      2.147      2.732         14        640: 100%|██████████| 110/110 [06:51<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.51s/it]\n",
      "                   all        574        104     0.0359      0.173     0.0323      0.016\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10         0G      1.854      1.976      2.576         14        640: 100%|██████████| 110/110 [06:49<00:00,  3.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:44<00:00,  2.50s/it]\n",
      "                   all        574        104     0.0572      0.375      0.045     0.0227\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10         0G      1.808      1.916        2.5         11        640: 100%|██████████| 110/110 [06:51<00:00,  3.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.51s/it]\n",
      "                   all        574        104     0.0488      0.308     0.0407      0.021\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10         0G      1.715      1.826      2.441         12        640: 100%|██████████| 110/110 [06:48<00:00,  3.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.50s/it]\n",
      "                   all        574        104      0.055      0.375     0.0448     0.0237\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10         0G      1.656      1.812      2.362         14        640: 100%|██████████| 110/110 [06:49<00:00,  3.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.51s/it]\n",
      "                   all        574        104     0.0598      0.394     0.0513     0.0274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10         0G      1.576      1.709      2.287         14        640: 100%|██████████| 110/110 [06:53<00:00,  3.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:45<00:00,  2.54s/it]\n",
      "                   all        574        104      0.048      0.317     0.0427      0.024\n",
      "\n",
      "10 epochs completed in 1.268 hours.\n",
      "c:\\Users\\ENGHK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\yolo\\utils\\torch_utils.py:400: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x = torch.load(f, map_location=torch.device('cpu'))\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.134  Python-3.11.5 torch-2.4.1+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "c:\\Users\\ENGHK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "YOLOv8n summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:38<00:00,  2.16s/it]\n",
      "                   all        574        104     0.0597      0.394     0.0514     0.0275\n",
      "Speed: 1.4ms preprocess, 47.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "c:\\Users\\ENGHK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n"
     ]
    }
   ],
   "source": [
    "# Load YOLO model\n",
    "yolo_model = YOLO('yolov8n.yaml')  # Ensure this file \"yolov8n.yaml\" exists and is configured correctly\n",
    "\n",
    "# Train YOLO model\n",
    "yolo_model.train(data=data_yaml_path, epochs=10, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Perform Inference and Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image: 00003385.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ENGHK\\Desktop\\Object Detection System - Human\\Filtered_Data\\Filtered_Images\\00003385.jpg: 608x640 7 persons, 118.0ms\n",
      "Speed: 4.0ms preprocess, 118.0ms inference, 0.0ms postprocess per image at shape (1, 3, 608, 640)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "model_path = r\"C:\\Users\\ENGHK\\Desktop\\Object Detection System - Human/output/yolov8n.pt\"\n",
    "image_dir = r\"C:\\Users\\ENGHK\\Desktop\\Object Detection System - Human/Filtered_Data/Filtered_Images/\"\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# List all images in the directory\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "# Randomly select an image\n",
    "selected_image = random.choice(image_files)\n",
    "image_path = os.path.join(image_dir, selected_image)\n",
    "\n",
    "# Print the name of the selected image\n",
    "print(f\"Selected image: {selected_image}\")\n",
    "\n",
    "# Perform inference on the selected image\n",
    "results = model(image_path)[0]\n",
    "\n",
    "# Function to draw bounding boxes\n",
    "def draw_boxes(img, boxes):\n",
    "    for box in boxes:\n",
    "        # If you only have x1, y1, x2, y2, extract these values\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "        # Optionally add labels if confidence and class_id are available\n",
    "        # For now, just displaying the coordinates\n",
    "        cv2.putText(img, f'{x1},{y1},{x2},{y2}', (x1, y1 - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)  # Label\n",
    "    return img\n",
    "\n",
    "# Load the selected image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Draw boxes on the image\n",
    "img_with_boxes = draw_boxes(img, results.boxes.xyxy.cpu().numpy())\n",
    "\n",
    "# Save the image with bounding boxes\n",
    "output_image_path = 'output_image_with_boxes.jpg'\n",
    "cv2.imwrite(output_image_path, img_with_boxes)\n",
    "\n",
    "# Read and display the saved image\n",
    "saved_img = cv2.imread(output_image_path)\n",
    "cv2.imshow('Image with Bounding Boxes', saved_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
